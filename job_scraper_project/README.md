# OSINT + Job Scraping Dashboard

[![CI/CD Pipeline](https://github.com/killo431/CrawlerLLM/actions/workflows/ci-cd.yml/badge.svg)](https://github.com/killo431/CrawlerLLM/actions)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A production-ready, modular job scraping system with OSINT capabilities. Extract job listings from multiple career sites, perform OSINT investigations, and generate new adapters using AI.

## âœ¨ Features

- **Job Scraping**: Extract job listings from Indeed, LinkedIn, and Glassdoor
- **OSINT Tools**: Phone lookup, digital footprint tracing, and email breach checking
- **AI-Powered**: LLM-based adapter generation for new sites
- **Stealth Capabilities**: Proxy rotation and fingerprint masking
- **Production Ready**: Docker support, CI/CD, comprehensive testing
- **Interactive Dashboard**: Streamlit UI for all features
- **CLI Interface**: Command-line tool for batch processing

## ğŸ“‹ Requirements

- Python 3.11 or higher
- Docker (optional, for containerized deployment)
- Playwright browsers (installed automatically)

## ğŸš€ Quick Start

### Using Make (Recommended)

```bash
# Install dependencies
make install

# Run the CLI scraper
make run

# Run the dashboard
make dashboard

# Run tests
make test

# Format code
make format
```

### Manual Installation

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   playwright install chromium
   ```

2. **Configure environment (optional):**
   ```bash
   cp .env.example .env
   # Edit .env with your settings
   ```

3. **Run the CLI:**
   ```bash
   python main.py
   ```

4. **Run the dashboard:**
   ```bash
   streamlit run dashboard/app.py
   ```

### Using Docker

```bash
# Build and run with docker-compose
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down

# Run CLI version
docker-compose --profile cli up job-scraper-cli
```

## ğŸ“ Project Structure

```
job_scraper_project/
â”œâ”€â”€ adapters/              # Site-specific scrapers
â”‚   â”œâ”€â”€ base_scraper.py   # Abstract base class
â”‚   â”œâ”€â”€ indeed.py         # Indeed scraper
â”‚   â”œâ”€â”€ linkedin.py       # LinkedIn scraper
â”‚   â””â”€â”€ glassdoor.py      # Glassdoor scraper
â”œâ”€â”€ core/                  # Core functionality
â”‚   â”œâ”€â”€ browser.py        # Playwright browser automation
â”‚   â”œâ”€â”€ logger.py         # Centralized logging
â”‚   â”œâ”€â”€ export_manager.py # Data export (JSON/CSV)
â”‚   â”œâ”€â”€ config.py         # Configuration management
â”‚   â”œâ”€â”€ environment.py    # Environment variables
â”‚   â”œâ”€â”€ proxy.py          # Proxy rotation
â”‚   â””â”€â”€ utils.py          # Utility functions
â”œâ”€â”€ scrapers/osint/       # OSINT tools
â”‚   â”œâ”€â”€ phone_lookup.py   # Phone number lookup
â”‚   â”œâ”€â”€ footprint_trace.py # Digital footprint tracing
â”‚   â””â”€â”€ breach_checker.py # Email breach checking
â”œâ”€â”€ ai_dev/               # AI features
â”‚   â””â”€â”€ feature_developer.py # Adapter generation
â”œâ”€â”€ dashboard/            # Streamlit dashboard
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ tests/                # Test suite
â”‚   â”œâ”€â”€ test_base_scraper.py
â”‚   â”œâ”€â”€ test_export_manager.py
â”‚   â”œâ”€â”€ test_utils.py
â”‚   â””â”€â”€ test_config.py
â”œâ”€â”€ data/output/          # Output data
â”œâ”€â”€ logs/                 # Log files
â”œâ”€â”€ config.yaml           # Configuration file
â”œâ”€â”€ .env.example          # Environment template
â”œâ”€â”€ Dockerfile            # Docker image
â”œâ”€â”€ docker-compose.yml    # Docker services
â”œâ”€â”€ Makefile              # Development tasks
â”œâ”€â”€ setup.py              # Package setup
â””â”€â”€ pyproject.toml        # Project metadata
```

## ğŸ¯ Usage

### Job Scraping (CLI)

```bash
# Run all scrapers
python main.py

# Results will be saved to data/output/
# - all_jobs.json
# - all_jobs.csv
```

### Job Scraping (Dashboard)

1. Open the dashboard: `streamlit run dashboard/app.py`
2. Navigate to "Job Scraping" tab
3. Select a job board (Indeed, LinkedIn, or Glassdoor)
4. Click "Start Scraping"
5. View and export results

### OSINT Tools

Access through the dashboard:

- **Phone Lookup**: Enter a phone number to get carrier and location info
- **Footprint Trace**: Enter a name to find associated online accounts
- **Breach Checker**: Enter an email to check for known data breaches

### AI Feature Developer

Generate new scraper adapters automatically:

1. Navigate to "AI Feature Developer" tab
2. Enter target site domain
3. Specify fields to extract
4. Generate adapter code
5. Save to adapters/ directory

## âš™ï¸ Configuration

### Using config.yaml

Edit `config.yaml` to customize:

```yaml
global:
  output_format: json
  log_level: INFO
  max_retries: 3

indeed:
  base_url: "https://www.indeed.com/jobs?q=python"
  crawl_delay: 2
  dynamic: false
```

### Using Environment Variables

Create a `.env` file from `.env.example`:

```bash
# Application Settings
LOG_LEVEL=INFO
OUTPUT_FORMAT=json
MAX_RETRIES=3
TIMEOUT=30

# API Keys (optional)
LINKEDIN_API_KEY=your_api_key_here
```

## ğŸ§ª Development

### Running Tests

```bash
# Run all tests
make test

# Run with coverage
pytest tests/ -v --cov=. --cov-report=html

# Run specific test file
pytest tests/test_base_scraper.py -v
```

### Code Quality

```bash
# Format code
make format

# Check formatting
make format-check

# Run linters
make lint

# Type checking
mypy . --ignore-missing-imports
```

### Development Setup

```bash
# Install development dependencies
make install-dev

# This installs:
# - pytest (testing)
# - black (code formatting)
# - flake8 (linting)
# - mypy (type checking)
# - isort (import sorting)
```

## ğŸ³ Docker Deployment

### Production Deployment

```bash
# Build image
docker build -t job-scraper:latest .

# Run dashboard
docker run -p 8501:8501 -v $(pwd)/data:/app/data job-scraper:latest

# Run CLI
docker run -v $(pwd)/data:/app/data job-scraper:latest python main.py
```

### Docker Compose

```bash
# Start services
docker-compose up -d

# Scale CLI workers
docker-compose --profile cli up --scale job-scraper-cli=3

# View logs
docker-compose logs -f job-scraper

# Stop services
docker-compose down
```

## ğŸ”’ Security

### Best Practices

- Store sensitive data in environment variables (`.env`)
- Never commit `.env` files to version control
- Use proxy rotation for production scraping
- Respect robots.txt and rate limits
- Implement proper error handling and logging

### Security Scanning

The project includes automated security scanning via GitHub Actions:

- Trivy vulnerability scanner
- Dependency vulnerability checks
- Code security analysis

## ğŸ“Š CI/CD

Automated pipeline includes:

- âœ… Unit testing across Python 3.11 and 3.12
- âœ… Code linting (flake8)
- âœ… Code formatting checks (black, isort)
- âœ… Type checking (mypy)
- âœ… Docker image building
- âœ… Security scanning (Trivy)

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) for details

## âš ï¸ Legal & Ethics

This tool is designed for **ethical use only**. Always respect:

- âœ“ robots.txt files
- âœ“ Terms of service
- âœ“ Rate limits
- âœ“ Privacy laws (GDPR, CCPA, etc.)
- âœ“ Copyright and data ownership

**Intended for:**
- Legitimate research purposes
- Authorized penetration testing
- Educational use
- Personal job search automation (with permission)

**Not intended for:**
- Unauthorized data scraping
- Terms of service violations
- Illegal activities
- Commercial data harvesting without permission

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Write tests for new features
4. Ensure all tests pass
5. Format code with black and isort
6. Submit a pull request

## ğŸ“ Support

For issues, questions, or contributions:
- GitHub Issues: [https://github.com/killo431/CrawlerLLM/issues](https://github.com/killo431/CrawlerLLM/issues)
- Documentation: See inline code documentation

## ğŸ™ Acknowledgments

This project uses:
- [Playwright](https://playwright.dev/) for browser automation
- [Streamlit](https://streamlit.io/) for the dashboard UI
- [Pydantic](https://pydantic-docs.helpmanual.io/) for data validation
- [Loguru](https://loguru.readthedocs.io/) for logging

---

**Project Status**: âœ… Production Ready  
**Version**: 1.0.0  
**Last Updated**: November 6, 2025
